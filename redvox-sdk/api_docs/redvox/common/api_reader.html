<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>redvox.common.api_reader API documentation</title>
<meta name="description" content="Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>redvox.common.api_reader</code></h1>
</header>
<section id="section-intro">
<p>Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats
&#34;&#34;&#34;
from typing import List, Optional
from datetime import timedelta, datetime
import multiprocessing
import multiprocessing.pool

import pyarrow as pa
import psutil

import redvox.settings as settings
import redvox.api1000.proto.redvox_api_m_pb2 as api_m
import redvox.common.date_time_utils as dtu
from redvox.common import io, api_conversions as ac
from redvox.common.parallel_utils import maybe_parallel_map
from redvox.common.station import Station
from redvox.common.reader_session_model import ModelsContainer
from redvox.common.session_model import SessionModel
from redvox.common.errors import RedVoxExceptions
from redvox.cloud.client import cloud_client
from redvox.cloud.session_model_api import Session
from redvox.cloud.errors import CloudApiError


id_py_stct = pa.struct(
    [
        (&#34;id&#34;, pa.string()),
        (&#34;uuid&#34;, pa.string()),
        (&#34;start_time&#34;, pa.float64()),
    ]
)
meta_py_stct = pa.struct(
    [
        (&#34;api&#34;, pa.float64()),
        (&#34;sub_api&#34;, pa.float64()),
        (&#34;make&#34;, pa.string()),
        (&#34;model&#34;, pa.string()),
        (&#34;os&#34;, pa.int64()),
        (&#34;os_version&#34;, pa.string()),
        (&#34;app&#34;, pa.string()),
        (&#34;app_version&#34;, pa.string()),
        (&#34;is_private&#34;, pa.bool_()),
        (&#34;packet_duration_s&#34;, pa.float64()),
        (&#34;station_description&#34;, pa.string()),
    ]
)


PERCENT_FREE_MEM_USE = 0.8  # Percentage of total free memory to use when creating stations (1. is 100%)


class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        session_models: ModelContainer for cloud and local session models.

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool

        if read_filter:
            self.filter: io.ReadFilter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir: str = base_dir
        self.structured_dir: bool = structured_dir
        self.debug: bool = debug
        self.errors: RedVoxExceptions = RedVoxExceptions(&#34;APIReader&#34;)
        self.session_models: ModelsContainer = ModelsContainer()
        self.files_index: List[io.Index] = self._get_all_files(_pool)
        self.index_summary: io.IndexSummary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            total_est_size = max_file_size * sum([len(fi.entries) for fi in self.files_index])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(
                    f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                    f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;
                )
            elif total_est_size / mem_split_factor &gt; self.chunk_limit:
                raise MemoryError(
                    f&#34;{total_est_size} of data requested, but only {self.chunk_limit} available; &#34;
                    f&#34;please reduce the amount of data you are requesting.&#34;
                )
            if debug:
                if mem_split_factor == 1:
                    print(
                        f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                        f&#34;bytes for loading files in memory.&#34;
                    )
                else:
                    print(
                        f&#34;{mem_split_factor} stations each have &#34;
                        f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;
                    )
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_cloud_models(self, ids: List[str]):
        &#34;&#34;&#34;
        saves the cloud models from the server that match the list of ids given to the ApiReader&#39;s session_models.
        :param ids: station ids to get models for
        &#34;&#34;&#34;
        try:
            with cloud_client() as client:
                self.session_models.search_cloud_session(
                    id_uuids=ids,
                    owner=client.redvox_config.username,
                    start_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.start_dt))
                    if self.filter.start_dt
                    else None,
                    end_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.end_dt))
                    if self.filter.end_dt
                    else None,
                    include_public=True,
                )
                if self.session_models.cloud_models is None:
                    self.errors.append(f&#34;Unable to find any cloud sessions for {ids}.  Using local files.&#34;)
        except CloudApiError as e:
            self.errors.append(f&#34;Error while connecting to server.  Error message: {e}&#34;)
        except Exception as e:
            self.errors.append(f&#34;An error occurred.  Error message: {e}&#34;)

    def _reset_index(self, model: Session) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        reset the filter used to get files, then get the updated list of files

        :param model: model to use to reset filter
        :return: updated index of files
        &#34;&#34;&#34;
        insufficient_str = &#34;&#34;
        # reset the filter used to get files
        new_filter = (
            io.ReadFilter()
            .with_extensions(self.filter.extensions)
            .with_api_versions(self.filter.api_versions)
            .with_station_ids({model.id})
            .with_start_dt_buf(dtu.timedelta(seconds=0))
            .with_end_dt_buf(dtu.timedelta(seconds=0))
        )
        # update the start and end times for the filter by the mean offset and the packet duration
        if self.filter.start_dt is not None:
            if timedelta(microseconds=abs(model.timing.mean_off)) &gt; self.filter.start_dt_buf:
                insufficient_str += &#34;start &#34;
            new_filter.with_start_dt(
                self.filter.start_dt + timedelta(microseconds=(model.timing.mean_off - model.packet_dur))
            )
        if self.filter.end_dt is not None:
            if timedelta(microseconds=abs(model.timing.mean_off)) &gt; self.filter.end_dt_buf:
                insufficient_str += &#34;end&#34;
            new_filter.with_end_dt(
                self.filter.end_dt + timedelta(microseconds=(model.timing.mean_off + model.packet_dur))
            )

        if len(insufficient_str) &gt; 0:
            self.errors.append(f&#34;Required more data for {model.id} at: {insufficient_str}&#34;)
        return [self._apply_filter(new_filter)]

    def _get_all_files(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        all_index_ids = all_index.summarize().station_ids()
        # get models using the cloud to correct timing
        self._get_cloud_models(all_index_ids)
        resp_ids = self.session_models.list_ids()

        for station_id in all_index_ids:
            # if start and end are both not defined, just use what we got
            if self.filter.start_dt is None and self.filter.end_dt is None:
                checked_index = [all_index.get_index_for_station_id(station_id)]
            # if we need to update the start or end, use the first session model from cloud if it exists
            elif station_id in resp_ids:
                checked_index = self._reset_index(self.session_models.get_model_by_key(station_id))
            # if no models from cloud, use the data available to update start and end of index
            else:
                id_index = all_index.get_index_for_station_id(station_id)
                if len(id_index.entries) &lt; 1:
                    checked_index = []
                else:
                    # attempt to make a session model using local data.  if failure, use what we got initially.
                    try:
                        stats = SessionModel().create_from_stream(self.read_files_in_index(id_index))
                        checked_index = self._reset_index(stats.cloud_session)
                        self.session_models.add_local_session(stats)
                    except (ValueError, Exception):
                        checked_index = [id_index]

            # add the updated list of files to the index
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(self, station_ids: set, new_start: datetime, new_end: datetime) -&gt; Optional[io.Index]:
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        new_index = self._apply_filter(
            io.ReadFilter()
            .with_start_dt(new_start)
            .with_end_dt(new_end)
            .with_extensions(self.filter.extensions)
            .with_api_versions(self.filter.api_versions)
            .with_station_ids(station_ids)
            .with_start_dt_buf(timedelta(seconds=0))
            .with_end_dt_buf(timedelta(seconds=0))
        )
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900}).with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000}).with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool, self._station_by_index, iter(self.files_index), chunk_size=1))

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="redvox.common.api_reader.ApiReader"><code class="flex name class">
<span>class <span class="ident">ApiReader</span></span>
<span>(</span><span>base_dir: str, structured_dir: bool = False, read_filter: <a title="redvox.common.io.ReadFilter" href="io.html#redvox.common.io.ReadFilter">ReadFilter</a> = None, debug: bool = False, pool: Optional[multiprocessing.pool.Pool] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
ease of comparison and use.</p>
<h2 id="properties">Properties</h2>
<p>filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
types of files to read</p>
<p>base_dir: str of the directory containing all the files to read</p>
<p>structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
respective api formats.
If False, base_dir only has the data files.
Default False.</p>
<p>files_index: io.Index of the files that match the filter that are in base_dir</p>
<p>index_summary: io.IndexSummary of the filtered data</p>
<p>session_models: ModelContainer for cloud and local session models.</p>
<p>debug: bool, if True, output additional information during function execution.
Default False.</p>
<p>Initialize the ApiReader object</p>
<p>:param base_dir: directory containing the files to read
:param structured_dir: if True, base_dir contains a specific directory structure used by the respective
api formats.
If False, base_dir only has the data files.
Default False.
:param read_filter: ReadFilter for the data files, if None, get everything.
Default None
:param debug: if True, output program warnings/errors during function execution.
Default False.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        session_models: ModelContainer for cloud and local session models.

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool

        if read_filter:
            self.filter: io.ReadFilter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir: str = base_dir
        self.structured_dir: bool = structured_dir
        self.debug: bool = debug
        self.errors: RedVoxExceptions = RedVoxExceptions(&#34;APIReader&#34;)
        self.session_models: ModelsContainer = ModelsContainer()
        self.files_index: List[io.Index] = self._get_all_files(_pool)
        self.index_summary: io.IndexSummary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            total_est_size = max_file_size * sum([len(fi.entries) for fi in self.files_index])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(
                    f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                    f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;
                )
            elif total_est_size / mem_split_factor &gt; self.chunk_limit:
                raise MemoryError(
                    f&#34;{total_est_size} of data requested, but only {self.chunk_limit} available; &#34;
                    f&#34;please reduce the amount of data you are requesting.&#34;
                )
            if debug:
                if mem_split_factor == 1:
                    print(
                        f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                        f&#34;bytes for loading files in memory.&#34;
                    )
                else:
                    print(
                        f&#34;{mem_split_factor} stations each have &#34;
                        f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;
                    )
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_cloud_models(self, ids: List[str]):
        &#34;&#34;&#34;
        saves the cloud models from the server that match the list of ids given to the ApiReader&#39;s session_models.
        :param ids: station ids to get models for
        &#34;&#34;&#34;
        try:
            with cloud_client() as client:
                self.session_models.search_cloud_session(
                    id_uuids=ids,
                    owner=client.redvox_config.username,
                    start_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.start_dt))
                    if self.filter.start_dt
                    else None,
                    end_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.end_dt))
                    if self.filter.end_dt
                    else None,
                    include_public=True,
                )
                if self.session_models.cloud_models is None:
                    self.errors.append(f&#34;Unable to find any cloud sessions for {ids}.  Using local files.&#34;)
        except CloudApiError as e:
            self.errors.append(f&#34;Error while connecting to server.  Error message: {e}&#34;)
        except Exception as e:
            self.errors.append(f&#34;An error occurred.  Error message: {e}&#34;)

    def _reset_index(self, model: Session) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        reset the filter used to get files, then get the updated list of files

        :param model: model to use to reset filter
        :return: updated index of files
        &#34;&#34;&#34;
        insufficient_str = &#34;&#34;
        # reset the filter used to get files
        new_filter = (
            io.ReadFilter()
            .with_extensions(self.filter.extensions)
            .with_api_versions(self.filter.api_versions)
            .with_station_ids({model.id})
            .with_start_dt_buf(dtu.timedelta(seconds=0))
            .with_end_dt_buf(dtu.timedelta(seconds=0))
        )
        # update the start and end times for the filter by the mean offset and the packet duration
        if self.filter.start_dt is not None:
            if timedelta(microseconds=abs(model.timing.mean_off)) &gt; self.filter.start_dt_buf:
                insufficient_str += &#34;start &#34;
            new_filter.with_start_dt(
                self.filter.start_dt + timedelta(microseconds=(model.timing.mean_off - model.packet_dur))
            )
        if self.filter.end_dt is not None:
            if timedelta(microseconds=abs(model.timing.mean_off)) &gt; self.filter.end_dt_buf:
                insufficient_str += &#34;end&#34;
            new_filter.with_end_dt(
                self.filter.end_dt + timedelta(microseconds=(model.timing.mean_off + model.packet_dur))
            )

        if len(insufficient_str) &gt; 0:
            self.errors.append(f&#34;Required more data for {model.id} at: {insufficient_str}&#34;)
        return [self._apply_filter(new_filter)]

    def _get_all_files(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        all_index_ids = all_index.summarize().station_ids()
        # get models using the cloud to correct timing
        self._get_cloud_models(all_index_ids)
        resp_ids = self.session_models.list_ids()

        for station_id in all_index_ids:
            # if start and end are both not defined, just use what we got
            if self.filter.start_dt is None and self.filter.end_dt is None:
                checked_index = [all_index.get_index_for_station_id(station_id)]
            # if we need to update the start or end, use the first session model from cloud if it exists
            elif station_id in resp_ids:
                checked_index = self._reset_index(self.session_models.get_model_by_key(station_id))
            # if no models from cloud, use the data available to update start and end of index
            else:
                id_index = all_index.get_index_for_station_id(station_id)
                if len(id_index.entries) &lt; 1:
                    checked_index = []
                else:
                    # attempt to make a session model using local data.  if failure, use what we got initially.
                    try:
                        stats = SessionModel().create_from_stream(self.read_files_in_index(id_index))
                        checked_index = self._reset_index(stats.cloud_session)
                        self.session_models.add_local_session(stats)
                    except (ValueError, Exception):
                        checked_index = [id_index]

            # add the updated list of files to the index
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(self, station_ids: set, new_start: datetime, new_end: datetime) -&gt; Optional[io.Index]:
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        new_index = self._apply_filter(
            io.ReadFilter()
            .with_start_dt(new_start)
            .with_end_dt(new_end)
            .with_extensions(self.filter.extensions)
            .with_api_versions(self.filter.api_versions)
            .with_station_ids(station_ids)
            .with_start_dt_buf(timedelta(seconds=0))
            .with_end_dt_buf(timedelta(seconds=0))
        )
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900}).with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000}).with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool, self._station_by_index, iter(self.files_index), chunk_size=1))

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="redvox.common.api_reader_dw.ApiReaderDw" href="api_reader_dw.html#redvox.common.api_reader_dw.ApiReaderDw">ApiReaderDw</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.read_files_in_index"><code class="name flex">
<span>def <span class="ident">read_files_in_index</span></span>(<span>indexf: <a title="redvox.common.io.Index" href="io.html#redvox.common.io.Index">Index</a>) ‑> List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]</span>
</code></dt>
<dd>
<div class="desc"><p>read all the files in the index</p>
<p>:return: list of RedvoxPacketM, converted from API 900 if necessary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
    &#34;&#34;&#34;
    read all the files in the index

    :return: list of RedvoxPacketM, converted from API 900 if necessary
    &#34;&#34;&#34;
    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    # noinspection PyTypeChecker
    for packet_900 in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})):
        # noinspection Mypy
        result.append(ac.convert_api_900_to_1000_raw(packet_900))

    # Grab the API 1000 packets
    # noinspection PyTypeChecker
    for packet in indexf.stream_raw(io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})):
        # noinspection Mypy
        result.append(packet)

    return result</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.get_station_by_id"><code class="name flex">
<span>def <span class="ident">get_station_by_id</span></span>(<span>self, get_id: str) ‑> Optional[List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param get_id: the id to filter on
:return: list of all stations with the requested id or None if id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
    &#34;&#34;&#34;
    :param get_id: the id to filter on
    :return: list of all stations with the requested id or None if id can&#39;t be found
    &#34;&#34;&#34;
    result = [s for s in self.get_stations() if s.id() == get_id]
    if len(result) &lt; 1:
        return None
    return result</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.get_stations"><code class="name flex">
<span>def <span class="ident">get_stations</span></span>(<span>self, pool: Optional[multiprocessing.pool.Pool] = None) ‑> List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>:param pool: optional multiprocessing pool
:return: List of all stations in the ApiReader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
    &#34;&#34;&#34;
    :param pool: optional multiprocessing pool
    :return: List of all stations in the ApiReader
    &#34;&#34;&#34;
    return list(maybe_parallel_map(pool, self._station_by_index, iter(self.files_index), chunk_size=1))</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.read_files_by_id"><code class="name flex">
<span>def <span class="ident">read_files_by_id</span></span>(<span>self, station_id: str) ‑> Optional[List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param station_id: the id to filter on
:return: the list of packets with the requested id, or None if the id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
    &#34;&#34;&#34;
    :param station_id: the id to filter on
    :return: the list of packets with the requested id, or None if the id can&#39;t be found
    &#34;&#34;&#34;

    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    for packet_900 in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900}).with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(ac.convert_api_900_to_1000_raw(packet_900))

    # Grab the API 1000 packets
    for packet in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000}).with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(packet)

    if len(result) == 0:
        return None

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="redvox.common" href="index.html">redvox.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="redvox.common.api_reader.ApiReader" href="#redvox.common.api_reader.ApiReader">ApiReader</a></code></h4>
<ul class="">
<li><code><a title="redvox.common.api_reader.ApiReader.get_station_by_id" href="#redvox.common.api_reader.ApiReader.get_station_by_id">get_station_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.get_stations" href="#redvox.common.api_reader.ApiReader.get_stations">get_stations</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_by_id" href="#redvox.common.api_reader.ApiReader.read_files_by_id">read_files_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_in_index" href="#redvox.common.api_reader.ApiReader.read_files_in_index">read_files_in_index</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>