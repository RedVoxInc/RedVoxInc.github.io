<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>redvox.common.api_reader API documentation</title>
<meta name="description" content="Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>redvox.common.api_reader</code></h1>
</header>
<section id="section-intro">
<p>Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats
&#34;&#34;&#34;
from typing import List, Optional
from datetime import timedelta, datetime
import multiprocessing
import multiprocessing.pool

import numpy as np
import pyarrow as pa
import psutil

import redvox.settings as settings
import redvox.api1000.proto.redvox_api_m_pb2 as api_m
import redvox.common.date_time_utils as dtu
from redvox.common import offset_model as om,\
    io,\
    api_conversions as ac,\
    file_statistics as fs
from redvox.common.parallel_utils import maybe_parallel_map
from redvox.common.station import Station
from redvox.common.session_model import SessionModel
from redvox.common.errors import RedVoxExceptions
from redvox.cloud.client import cloud_client
from redvox.cloud.session_model_api import SessionModelsResp
from redvox.cloud.errors import CloudApiError


id_py_stct = pa.struct([(&#34;id&#34;, pa.string()), (&#34;uuid&#34;, pa.string()), (&#34;start_time&#34;, pa.float64()),
                        ])
meta_py_stct = pa.struct([(&#34;api&#34;, pa.float64()), (&#34;sub_api&#34;, pa.float64()), (&#34;make&#34;, pa.string()),
                          (&#34;model&#34;, pa.string()), (&#34;os&#34;, pa.int64()), (&#34;os_version&#34;, pa.string()),
                          (&#34;app&#34;, pa.string()), (&#34;app_version&#34;, pa.string()), (&#34;is_private&#34;, pa.bool_()),
                          (&#34;packet_duration_s&#34;, pa.float64()), (&#34;station_description&#34;, pa.string()),
                          ])


PERCENT_FREE_MEM_USE = .8  # Percentage of total free memory to use when creating stations (1. is 100%)


class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            total_est_size = max_file_size * sum([len(fi.entries) for fi in self.files_index])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                                  f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;)
            elif total_est_size / mem_split_factor &gt; self.chunk_limit:
                raise MemoryError(f&#34;{total_est_size} of data requested, but only {self.chunk_limit} available; &#34;
                                  f&#34;please reduce the amount of data you are requesting.&#34;)
            if debug:
                if mem_split_factor == 1:
                    print(f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                          f&#34;bytes for loading files in memory.&#34;)
                else:
                    print(f&#34;{mem_split_factor} stations each have &#34;
                          f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;)
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_all_files(
        self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        all_index_ids = all_index.summarize().station_ids()
        # get models using the cloud to correct timing
        resp_ids = []
        try:
            with cloud_client() as client:
                resp: SessionModelsResp = client.request_session_models(
                    owner=client.redvox_config.username,
                    id_uuids=all_index_ids,
                    start_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.start_dt))
                    if self.filter.start_dt else None,
                    end_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.end_dt))
                    if self.filter.end_dt else None
                )
                if len(resp.sessions) &lt; 1:
                    self.errors.append(&#34;Unable to find sessions.  Check if the requested stations: belong to your &#34;
                                       &#34;account or are public or if the request time starts after 30 April 2023.&#34;)
                else:
                    resp_ids = [n.id for n in resp.sessions]
        except CloudApiError as e:
            self.errors.append(f&#34;Error while connecting to server.  Error message: {e}&#34;)
        except Exception as e:
            self.errors.append(f&#34;An error occurred.  Error message: {e}&#34;)

        for station_id in all_index_ids:
            # if start and end are both not defined, just use what we got
            if self.filter.start_dt is None and self.filter.end_dt is None:
                checked_index = [all_index.get_index_for_station_id(station_id)]
            # if we need to update the start or end, use the session model from cloud if it exists
            elif station_id in resp_ids:
                # use the first model available to update the index
                model = [n for n in resp.sessions if n.id == station_id][0]
                # reset the filter used to get files
                new_filter = io.ReadFilter() \
                    .with_extensions(self.filter.extensions) \
                    .with_api_versions(self.filter.api_versions) \
                    .with_station_ids({station_id}) \
                    .with_start_dt_buf(dtu.timedelta(seconds=0)) \
                    .with_end_dt_buf(dtu.timedelta(seconds=0))
                # update the start and end times for the filter by the mean offset and the packet duration
                if self.filter.start_dt is not None:
                    new_filter.with_start_dt(self.filter.start_dt
                                             + timedelta(microseconds=(model.timing.mean_off - model.packet_dur)))
                if self.filter.end_dt is not None:
                    new_filter.with_end_dt(self.filter.end_dt
                                           + timedelta(microseconds=(model.timing.mean_off + model.packet_dur)))
                # look for the files using the updated filter.  if both start and end is None, this will return
                # the same results as before.  However, the outside if statement already covers this case, with
                # or without the cloud&#39;s session model.
                checked_index = [self._apply_filter(new_filter)]
            # if no models from cloud, use the data available to update start and end of index
            else:
                id_index = all_index.get_index_for_station_id(station_id)
                # check if there are any entries for the station_id
                if len(id_index.entries) &lt; 1:
                    checked_index = []
                else:
                    try:
                        stats = SessionModel().create_from_stream(self.read_files_in_index(id_index))
                        mean_offset = stats.cloud_session.timing.mean_off
                        insufficient_str = &#34;&#34;
                        # if our filtered files do not encompass the request even when the packet times are updated
                        # try getting the difference of the expected start/end and the start/end of the data plus one
                        # packet
                        new_filter = io.ReadFilter() \
                            .with_extensions(self.filter.extensions) \
                            .with_api_versions(self.filter.api_versions) \
                            .with_station_ids({stats.cloud_session.id}) \
                            .with_start_dt_buf(dtu.timedelta(seconds=0)) \
                            .with_end_dt_buf(dtu.timedelta(seconds=0))
                        # update the start and end times for the filter by the mean offset and the packet duration
                        if self.filter.start_dt is not None \
                                and timedelta(microseconds=-mean_offset) &gt; self.filter.start_dt_buf:
                            new_filter.with_start_dt(self.filter.start_dt
                                                     + timedelta(microseconds=(stats.cloud_session.timing.mean_off
                                                                               - stats.cloud_session.packet_dur)))
                            insufficient_str += &#34;start&#34;
                        if self.filter.end_dt is not None \
                                and timedelta(microseconds=mean_offset) &gt; self.filter.end_dt_buf:
                            new_filter.with_end_dt(self.filter.end_dt
                                                   + timedelta(microseconds=(stats.cloud_session.timing.mean_off
                                                                             + stats.cloud_session.packet_dur)))
                            insufficient_str += &#34;end&#34;
                        # look for the files using the updated filter
                        checked_index = [self._apply_filter(new_filter)]

                        if len(insufficient_str) &gt; 0:
                            self.errors.append(f&#34;Required more data for {station_id} at: {insufficient_str}&#34;)
                    except Exception as e:
                        checked_index = [id_index]

            # add the updated list of files to the index
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(
            self,
            station_ids: set,
            new_start: datetime,
            new_end: datetime
    ) -&gt; Optional[io.Index]:
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        diff_s = diff_e = timedelta(seconds=0)
        new_index = self._apply_filter(io.ReadFilter()
                                       .with_start_dt(new_start)
                                       .with_end_dt(new_end)
                                       .with_extensions(self.filter.extensions)
                                       .with_api_versions(self.filter.api_versions)
                                       .with_station_ids(station_ids)
                                       .with_start_dt_buf(diff_s)
                                       .with_end_dt_buf(diff_e))
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
        ):
            # noinspection Mypy
            result.append(
                ac.convert_api_900_to_1000_raw(packet_900)
            )

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
        ):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_900})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_1000})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool,
                                       self._station_by_index,
                                       iter(self.files_index),
                                       chunk_size=1
                                       )
                    )

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result


# This class uses a slightly stripped down version of the above ApiReader meant to quickly create SessionModels.
# Some of the checks used in ApiReader are not necessary when creating SessionModel, while the core functionality
#  of finding and reading files remains.
class ApiReaderModel:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.
    Creates SessionModels for each station session within the data read.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.

        errors: RedVoxExceptions, a container for any errors encountered during function execution.
    &#34;&#34;&#34;

    def __init__(
            self,
            base_dir: str,
            structured_dir: bool = False,
            read_filter: io.ReadFilter = None,
            debug: bool = False,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.session_models: List[SessionModel] = []
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_session(self, s_id: str, uuid: str, start_date: float) -&gt; Optional[SessionModel]:
        &#34;&#34;&#34;
        get a session that matches all the inputs or None if no match

        :param s_id: station id to get
        :param uuid: station uuid to get
        :param start_date: station start date to get
        :return: SessionModel or None
        &#34;&#34;&#34;
        for m in self.session_models:
            if m.cloud_session.id == s_id and m.cloud_session.uuid == uuid and m.cloud_session.start_date == start_date:
                return m
        return None

    def _get_all_files(
            self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        for station_id in all_index.summarize().station_ids():
            id_index = all_index.get_index_for_station_id(station_id)
            for f in id_index.read_contents():
                sd = f.timing_information.app_start_mach_timestamp
                uid = f.station_information.uuid
                n = self._get_session(station_id, uid, sd)
                n.create_from_packet(f) if n is not None \
                    else self.session_models.append(SessionModel.create_from_packet(f))
            index.append(id_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
            self,
            reader_filter: Optional[io.ReadFilter] = None,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _check_station_stats(
            self,
            station_index: io.Index,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        check the index&#39;s results; if it has enough information, return it, otherwise search for more data.
        The index should only request one station id
        If the station was restarted during the request period, a new group of indexes will be created
        to represent the change in station metadata.
        Creates SessionModels for each station session found in the requested data.

        :param station_index: index representing the requested information
        :return: List of Indexes that includes as much information as possible that fits the request
        &#34;&#34;&#34;
        # if we found nothing, return the index
        if len(station_index.entries) &lt; 1:
            return [station_index]

        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool

        stats = fs.extract_stats(station_index, pool=_pool)
        # Close pool if created here
        if pool is None:
            _pool.close()

        results = {}

        for v, e in enumerate(stats):
            key = e.app_start_dt
            if key not in results.keys():
                results[key] = io.Index()
            results[key].append(entries=iter([station_index.entries[v]]))

        for s in results.values():
            m = SessionModel.create_from_stream(s.read_contents())
            self.session_models.append(m)

        return list(results.values())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="redvox.common.api_reader.ApiReader"><code class="flex name class">
<span>class <span class="ident">ApiReader</span></span>
<span>(</span><span>base_dir: str, structured_dir: bool = False, read_filter: <a title="redvox.common.io.ReadFilter" href="io.html#redvox.common.io.ReadFilter">ReadFilter</a> = None, debug: bool = False, pool: Optional[multiprocessing.pool.Pool] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
ease of comparison and use.</p>
<h2 id="properties">Properties</h2>
<p>filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
types of files to read</p>
<p>base_dir: str of the directory containing all the files to read</p>
<p>structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
respective api formats.
If False, base_dir only has the data files.
Default False.</p>
<p>files_index: io.Index of the files that match the filter that are in base_dir</p>
<p>index_summary: io.IndexSummary of the filtered data</p>
<p>debug: bool, if True, output additional information during function execution.
Default False.</p>
<p>Initialize the ApiReader object</p>
<p>:param base_dir: directory containing the files to read
:param structured_dir: if True, base_dir contains a specific directory structure used by the respective
api formats.
If False, base_dir only has the data files.
Default False.
:param read_filter: ReadFilter for the data files, if None, get everything.
Default None
:param debug: if True, output program warnings/errors during function execution.
Default False.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            total_est_size = max_file_size * sum([len(fi.entries) for fi in self.files_index])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                                  f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;)
            elif total_est_size / mem_split_factor &gt; self.chunk_limit:
                raise MemoryError(f&#34;{total_est_size} of data requested, but only {self.chunk_limit} available; &#34;
                                  f&#34;please reduce the amount of data you are requesting.&#34;)
            if debug:
                if mem_split_factor == 1:
                    print(f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                          f&#34;bytes for loading files in memory.&#34;)
                else:
                    print(f&#34;{mem_split_factor} stations each have &#34;
                          f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;)
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_all_files(
        self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        all_index_ids = all_index.summarize().station_ids()
        # get models using the cloud to correct timing
        resp_ids = []
        try:
            with cloud_client() as client:
                resp: SessionModelsResp = client.request_session_models(
                    owner=client.redvox_config.username,
                    id_uuids=all_index_ids,
                    start_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.start_dt))
                    if self.filter.start_dt else None,
                    end_ts=int(dtu.datetime_to_epoch_microseconds_utc(self.filter.end_dt))
                    if self.filter.end_dt else None
                )
                if len(resp.sessions) &lt; 1:
                    self.errors.append(&#34;Unable to find sessions.  Check if the requested stations: belong to your &#34;
                                       &#34;account or are public or if the request time starts after 30 April 2023.&#34;)
                else:
                    resp_ids = [n.id for n in resp.sessions]
        except CloudApiError as e:
            self.errors.append(f&#34;Error while connecting to server.  Error message: {e}&#34;)
        except Exception as e:
            self.errors.append(f&#34;An error occurred.  Error message: {e}&#34;)

        for station_id in all_index_ids:
            # if start and end are both not defined, just use what we got
            if self.filter.start_dt is None and self.filter.end_dt is None:
                checked_index = [all_index.get_index_for_station_id(station_id)]
            # if we need to update the start or end, use the session model from cloud if it exists
            elif station_id in resp_ids:
                # use the first model available to update the index
                model = [n for n in resp.sessions if n.id == station_id][0]
                # reset the filter used to get files
                new_filter = io.ReadFilter() \
                    .with_extensions(self.filter.extensions) \
                    .with_api_versions(self.filter.api_versions) \
                    .with_station_ids({station_id}) \
                    .with_start_dt_buf(dtu.timedelta(seconds=0)) \
                    .with_end_dt_buf(dtu.timedelta(seconds=0))
                # update the start and end times for the filter by the mean offset and the packet duration
                if self.filter.start_dt is not None:
                    new_filter.with_start_dt(self.filter.start_dt
                                             + timedelta(microseconds=(model.timing.mean_off - model.packet_dur)))
                if self.filter.end_dt is not None:
                    new_filter.with_end_dt(self.filter.end_dt
                                           + timedelta(microseconds=(model.timing.mean_off + model.packet_dur)))
                # look for the files using the updated filter.  if both start and end is None, this will return
                # the same results as before.  However, the outside if statement already covers this case, with
                # or without the cloud&#39;s session model.
                checked_index = [self._apply_filter(new_filter)]
            # if no models from cloud, use the data available to update start and end of index
            else:
                id_index = all_index.get_index_for_station_id(station_id)
                # check if there are any entries for the station_id
                if len(id_index.entries) &lt; 1:
                    checked_index = []
                else:
                    try:
                        stats = SessionModel().create_from_stream(self.read_files_in_index(id_index))
                        mean_offset = stats.cloud_session.timing.mean_off
                        insufficient_str = &#34;&#34;
                        # if our filtered files do not encompass the request even when the packet times are updated
                        # try getting the difference of the expected start/end and the start/end of the data plus one
                        # packet
                        new_filter = io.ReadFilter() \
                            .with_extensions(self.filter.extensions) \
                            .with_api_versions(self.filter.api_versions) \
                            .with_station_ids({stats.cloud_session.id}) \
                            .with_start_dt_buf(dtu.timedelta(seconds=0)) \
                            .with_end_dt_buf(dtu.timedelta(seconds=0))
                        # update the start and end times for the filter by the mean offset and the packet duration
                        if self.filter.start_dt is not None \
                                and timedelta(microseconds=-mean_offset) &gt; self.filter.start_dt_buf:
                            new_filter.with_start_dt(self.filter.start_dt
                                                     + timedelta(microseconds=(stats.cloud_session.timing.mean_off
                                                                               - stats.cloud_session.packet_dur)))
                            insufficient_str += &#34;start&#34;
                        if self.filter.end_dt is not None \
                                and timedelta(microseconds=mean_offset) &gt; self.filter.end_dt_buf:
                            new_filter.with_end_dt(self.filter.end_dt
                                                   + timedelta(microseconds=(stats.cloud_session.timing.mean_off
                                                                             + stats.cloud_session.packet_dur)))
                            insufficient_str += &#34;end&#34;
                        # look for the files using the updated filter
                        checked_index = [self._apply_filter(new_filter)]

                        if len(insufficient_str) &gt; 0:
                            self.errors.append(f&#34;Required more data for {station_id} at: {insufficient_str}&#34;)
                    except Exception as e:
                        checked_index = [id_index]

            # add the updated list of files to the index
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(
            self,
            station_ids: set,
            new_start: datetime,
            new_end: datetime
    ) -&gt; Optional[io.Index]:
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        diff_s = diff_e = timedelta(seconds=0)
        new_index = self._apply_filter(io.ReadFilter()
                                       .with_start_dt(new_start)
                                       .with_end_dt(new_end)
                                       .with_extensions(self.filter.extensions)
                                       .with_api_versions(self.filter.api_versions)
                                       .with_station_ids(station_ids)
                                       .with_start_dt_buf(diff_s)
                                       .with_end_dt_buf(diff_e))
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
        ):
            # noinspection Mypy
            result.append(
                ac.convert_api_900_to_1000_raw(packet_900)
            )

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
        ):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_900})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_1000})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool,
                                       self._station_by_index,
                                       iter(self.files_index),
                                       chunk_size=1
                                       )
                    )

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="redvox.common.api_reader_dw.ApiReaderDw" href="api_reader_dw.html#redvox.common.api_reader_dw.ApiReaderDw">ApiReaderDw</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.read_files_in_index"><code class="name flex">
<span>def <span class="ident">read_files_in_index</span></span>(<span>indexf: <a title="redvox.common.io.Index" href="io.html#redvox.common.io.Index">Index</a>) ‑> List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]</span>
</code></dt>
<dd>
<div class="desc"><p>read all the files in the index</p>
<p>:return: list of RedvoxPacketM, converted from API 900 if necessary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
    &#34;&#34;&#34;
    read all the files in the index

    :return: list of RedvoxPacketM, converted from API 900 if necessary
    &#34;&#34;&#34;
    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    # noinspection PyTypeChecker
    for packet_900 in indexf.stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
    ):
        # noinspection Mypy
        result.append(
            ac.convert_api_900_to_1000_raw(packet_900)
        )

    # Grab the API 1000 packets
    # noinspection PyTypeChecker
    for packet in indexf.stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
    ):
        # noinspection Mypy
        result.append(packet)

    return result</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.get_station_by_id"><code class="name flex">
<span>def <span class="ident">get_station_by_id</span></span>(<span>self, get_id: str) ‑> Optional[List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param get_id: the id to filter on
:return: list of all stations with the requested id or None if id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
    &#34;&#34;&#34;
    :param get_id: the id to filter on
    :return: list of all stations with the requested id or None if id can&#39;t be found
    &#34;&#34;&#34;
    result = [s for s in self.get_stations() if s.id() == get_id]
    if len(result) &lt; 1:
        return None
    return result</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.get_stations"><code class="name flex">
<span>def <span class="ident">get_stations</span></span>(<span>self, pool: Optional[multiprocessing.pool.Pool] = None) ‑> List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>:param pool: optional multiprocessing pool
:return: List of all stations in the ApiReader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
    &#34;&#34;&#34;
    :param pool: optional multiprocessing pool
    :return: List of all stations in the ApiReader
    &#34;&#34;&#34;
    return list(maybe_parallel_map(pool,
                                   self._station_by_index,
                                   iter(self.files_index),
                                   chunk_size=1
                                   )
                )</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.read_files_by_id"><code class="name flex">
<span>def <span class="ident">read_files_by_id</span></span>(<span>self, station_id: str) ‑> Optional[List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param station_id: the id to filter on
:return: the list of packets with the requested id, or None if the id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
    &#34;&#34;&#34;
    :param station_id: the id to filter on
    :return: the list of packets with the requested id, or None if the id can&#39;t be found
    &#34;&#34;&#34;

    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    for packet_900 in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty()
        .with_api_versions({io.ApiVersion.API_900})
        .with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(ac.convert_api_900_to_1000_raw(packet_900))

    # Grab the API 1000 packets
    for packet in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty()
        .with_api_versions({io.ApiVersion.API_1000})
        .with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(packet)

    if len(result) == 0:
        return None

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="redvox.common.api_reader.ApiReaderModel"><code class="flex name class">
<span>class <span class="ident">ApiReaderModel</span></span>
<span>(</span><span>base_dir: str, structured_dir: bool = False, read_filter: <a title="redvox.common.io.ReadFilter" href="io.html#redvox.common.io.ReadFilter">ReadFilter</a> = None, debug: bool = False, pool: Optional[multiprocessing.pool.Pool] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
ease of comparison and use.
Creates SessionModels for each station session within the data read.</p>
<h2 id="properties">Properties</h2>
<p>filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
types of files to read</p>
<p>base_dir: str of the directory containing all the files to read</p>
<p>structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
respective api formats.
If False, base_dir only has the data files.
Default False.</p>
<p>files_index: io.Index of the files that match the filter that are in base_dir</p>
<p>index_summary: io.IndexSummary of the filtered data</p>
<p>debug: bool, if True, output additional information during function execution.
Default False.</p>
<p>errors: RedVoxExceptions, a container for any errors encountered during function execution.</p>
<p>Initialize the ApiReader object</p>
<p>:param base_dir: directory containing the files to read
:param structured_dir: if True, base_dir contains a specific directory structure used by the respective
api formats.
If False, base_dir only has the data files.
Default False.
:param read_filter: ReadFilter for the data files, if None, get everything.
Default None
:param debug: if True, output program warnings/errors during function execution.
Default False.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiReaderModel:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.
    Creates SessionModels for each station session within the data read.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.

        errors: RedVoxExceptions, a container for any errors encountered during function execution.
    &#34;&#34;&#34;

    def __init__(
            self,
            base_dir: str,
            structured_dir: bool = False,
            read_filter: io.ReadFilter = None,
            debug: bool = False,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.session_models: List[SessionModel] = []
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(iter(i.entries))
        return result

    def _get_session(self, s_id: str, uuid: str, start_date: float) -&gt; Optional[SessionModel]:
        &#34;&#34;&#34;
        get a session that matches all the inputs or None if no match

        :param s_id: station id to get
        :param uuid: station uuid to get
        :param start_date: station start date to get
        :return: SessionModel or None
        &#34;&#34;&#34;
        for m in self.session_models:
            if m.cloud_session.id == s_id and m.cloud_session.uuid == uuid and m.cloud_session.start_date == start_date:
                return m
        return None

    def _get_all_files(
            self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        for station_id in all_index.summarize().station_ids():
            id_index = all_index.get_index_for_station_id(station_id)
            for f in id_index.read_contents():
                sd = f.timing_information.app_start_mach_timestamp
                uid = f.station_information.uuid
                n = self._get_session(station_id, uid, sd)
                n.create_from_packet(f) if n is not None \
                    else self.session_models.append(SessionModel.create_from_packet(f))
            index.append(id_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
            self,
            reader_filter: Optional[io.ReadFilter] = None,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _check_station_stats(
            self,
            station_index: io.Index,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        check the index&#39;s results; if it has enough information, return it, otherwise search for more data.
        The index should only request one station id
        If the station was restarted during the request period, a new group of indexes will be created
        to represent the change in station metadata.
        Creates SessionModels for each station session found in the requested data.

        :param station_index: index representing the requested information
        :return: List of Indexes that includes as much information as possible that fits the request
        &#34;&#34;&#34;
        # if we found nothing, return the index
        if len(station_index.entries) &lt; 1:
            return [station_index]

        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool

        stats = fs.extract_stats(station_index, pool=_pool)
        # Close pool if created here
        if pool is None:
            _pool.close()

        results = {}

        for v, e in enumerate(stats):
            key = e.app_start_dt
            if key not in results.keys():
                results[key] = io.Index()
            results[key].append(entries=iter([station_index.entries[v]]))

        for s in results.values():
            m = SessionModel.create_from_stream(s.read_contents())
            self.session_models.append(m)

        return list(results.values())</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="redvox.common" href="index.html">redvox.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="redvox.common.api_reader.ApiReader" href="#redvox.common.api_reader.ApiReader">ApiReader</a></code></h4>
<ul class="">
<li><code><a title="redvox.common.api_reader.ApiReader.get_station_by_id" href="#redvox.common.api_reader.ApiReader.get_station_by_id">get_station_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.get_stations" href="#redvox.common.api_reader.ApiReader.get_stations">get_stations</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_by_id" href="#redvox.common.api_reader.ApiReader.read_files_by_id">read_files_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_in_index" href="#redvox.common.api_reader.ApiReader.read_files_in_index">read_files_in_index</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="redvox.common.api_reader.ApiReaderModel" href="#redvox.common.api_reader.ApiReaderModel">ApiReaderModel</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>